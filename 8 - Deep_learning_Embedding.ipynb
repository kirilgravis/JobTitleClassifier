{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job title prediction with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys; sys.path.append('../')                                                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.4.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cufflinks as cf; cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization, Embedding, Dense, GlobalAveragePooling1D, Dropout, Reshape, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExtJobTitleText</th>\n",
       "      <th>JobTitle</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15045</th>\n",
       "      <td>Waste Collection / Garbage Truck Helper</td>\n",
       "      <td>Sanitation Worker</td>\n",
       "      <td>Waste Collection / Garbage Truck Helper gather garbage and other discarded materials set out by customers along designated routes in urban and rural communities and transport the materials to sanitary landfills or incinerator plants for disposal....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>Nurse Practitioner - Hourly</td>\n",
       "      <td>Nurse Practitioner (NP)</td>\n",
       "      <td>Corizon Health is the pioneer provider of correctional healthcare in the United States.  We are a company built on more than 40-years of innovation and experience in the industry. Our people, our practices and our commitment to success are the tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ExtJobTitleText                 JobTitle  \\\n",
       "15045  Waste Collection / Garbage Truck Helper        Sanitation Worker   \n",
       "1698               Nurse Practitioner - Hourly  Nurse Practitioner (NP)   \n",
       "\n",
       "                                                                                                                                                                                                                                                     Description  \n",
       "15045  Waste Collection / Garbage Truck Helper gather garbage and other discarded materials set out by customers along designated routes in urban and rural communities and transport the materials to sanitary landfills or incinerator plants for disposal....  \n",
       "1698   Corizon Health is the pioneer provider of correctional healthcare in the United States.  We are a company built on more than 40-years of innovation and experience in the industry. Our people, our practices and our commitment to success are the tr...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos = pd.read_csv('datasets/data_clean.csv')\n",
    "df_pos.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25405, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos[\"JobTitle_tokenized\"] = pd.factorize(df_pos.JobTitle)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pos[\"ext_job_title_tokenized\"] = pd.factorize(df_pos.ExtJobTitleText)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Warehouse Worker</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Packager</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pediatric Speech Language Pathologist</th>\n",
       "      <td>958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Retail Sales Representative</th>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Registered Nurse (RN)</th>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maintenance Planner</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hospital Admissions Coordinator</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Finance Manager</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saw Operator</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sewing Machine Operator</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>352 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       count\n",
       "Warehouse Worker                        1000\n",
       "Packager                                1000\n",
       "Pediatric Speech Language Pathologist    958\n",
       "Retail Sales Representative              810\n",
       "Registered Nurse (RN)                    701\n",
       "...                                      ...\n",
       "Maintenance Planner                       10\n",
       "Hospital Admissions Coordinator           10\n",
       "Finance Manager                           10\n",
       "Saw Operator                              10\n",
       "Sewing Machine Operator                   10\n",
       "\n",
       "[352 rows x 1 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos.JobTitle.value_counts().to_frame(name='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train and test set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = df_pos[df_pos['Description'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col, target_col = 'Description', 'JobTitle'\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "label_as_binary = LabelBinarizer()\n",
    "\n",
    "label_as_binary.fit(df_pos[target_col])\n",
    "\n",
    "training_set = df_pos[[text_col, target_col]].sample(frac=0.8, random_state=41)\n",
    "test_set = df_pos[~df_pos.index.isin(training_set.index)][[text_col, target_col]]\n",
    "\n",
    "train__y_labels = label_as_binary.transform(training_set[target_col])\n",
    "test__y_labels = label_as_binary.transform(test_set[target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(training_set) + len(test_set) == len(df_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Embedding with pooling and all words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a good size for the sequence_length? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50    225.00\n",
       "0.60    285.00\n",
       "0.70    334.00\n",
       "0.80    386.00\n",
       "0.90    516.00\n",
       "0.95    607.00\n",
       "0.99    942.94\n",
       "Name: Description, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos.Description.apply(lambda x: len(x.split(' '))).quantile([0.5,0.6,0.7,0.8,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a good size for the vocabulary? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57131\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_transformer = CountVectorizer().fit(df_pos['Description'])\n",
    "\n",
    "# Print total number of vocab words\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "sequence_length = 516\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to integers. Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    #standardize=lambda text: tf.strings.lower(text), # You can use your own normalization function here\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    name = 'Text_processing',\n",
    "    output_sequence_length=sequence_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(training_set[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPG: We protect and beautify the world. At PPG, we work every day to develop and deliver the paints, coatings and materials that our customers have trusted for more than 130 years. Through dedication and creativity, we solve our customers’ biggest challenges, collaborating closely to find the right path forward. With headquarters in Pittsburgh, we operate and innovate in more than 70 countries. We serve customers in construction, consumer products, industrial and transportation markets and aftermarkets. To learn more, visit www.ppg.com and follow @PPG on Twitter.About YouDoes the thought of sitting still all day make you want to scream? Are you a people person? Are you passionate about learning new things and sharing your new acquired knowledge with others?If you screamed “YES!” to each of those questions, you may be just who we are looking for to help our team “protect and beautify the world!”As a Sales Associate, you will be the reason our customers come back! They will be excited to see your friendly face because of the exceptional, timely customer service you will provide and the meaningful relationships you will develop with our regular paint contractors and new customers every day.Your assistance will help shape our customers’ lives. Whether they are picking out a color to paint a new nursery of first time parents, fulfilling a paint order for the new retirement home to help make their residents feel more at home, or helping newlyweds decide what color would best compliment their furniture in their first home together, you will make a lasting impression on people’s lives every day!Additional requirements of this entry level role include:Forklift Experience RequiredAbility to work flexible retail hours with varied shifts including weekends and holidaysValid Driver’s licenseAbility to lift up to 80 lbs. infrequently and 40 – 60 pounds routinelyComputer and internet applications proficiencyWhy join us:With PPG, you will find meaning in your work every day, and engage in opportunities that will shape you, personally and professionally. Your personal strengths will empower you to succeed and make an impact from day one.You will be inspired to learn and grow, and to get the support you need to identify and achieve your boldest career aspirations.Your passion to excel will be fueled by your connection to world-class partners, industry experts, the best and brightest colleagues, and future forward technologies.Your contributions will not only meet the challenges of our global customers, but help theme propel their industries forward.You will be welcomed into a culture where everyone’s ideas and contributions are valued and encouraged.Just like you, we are driven to make a difference in our world. PPG prides itself on the quality of its employees and as such, candidates who receive a job offer will be required to successfully pass a hair drug/toxins test and a background check. PPG offers an opportunity to grow and develop your career in an environment that provides a fulfilling workplace for employees, creates an environment for continuous learning, and embraces the ideas and diversity of others. All qualified applicants will receive consideration for employment without regard to sex, pregnancy, race, color, creed, religion, national origin, age, disability status, protected veteran status, marital status, sexual orientation, gender identity or expression, or any other legally protected status. PPG is an Equal Opportunity Employer. You may request a copy of PPG’s affirmative action plan by emailing ppgaap@ppg.com. To read more about Equal Employment Opportunity please see attached links: http://www1.eeoc.gov/employers/upload/eeoc_self_print_poster.pdfhttps://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdfhttps://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(516,), dtype=int64, numpy=\n",
       "array([  105,    14,   821,     2,   580,     4,   186,    26,   105,\n",
       "          14,    17,    99,    36,     3,   217,     2,   398,     4,\n",
       "        1099,   952,     2,   291,    22,     9,    57,    30,   933,\n",
       "           7,    35,   203,  1030,    44,    87,   405,     2,  1063,\n",
       "          14,   641,     9,   445,  1142,   384,   967,   744,     3,\n",
       "         167,     4,   350,   798,   120,    11,  1087,     8,  1077,\n",
       "          14,   434,     2,  1102,     8,    35,   203,   986,   804,\n",
       "          14,   511,    57,     8,   395,   972,    94,   453,     2,\n",
       "         599,   841,     2,  1101,     3,   166,    35,   669,  1176,\n",
       "           2,   527,   105,    23,  6596,  8227,     4,  1111,     6,\n",
       "         978,  1397,    32,    36,    42,    10,   512,     3,  1542,\n",
       "          13,    10,     5,    72,   423,    13,    10,   347,    81,\n",
       "         464,    77,   396,     2,  1407,    21,    77,  1531,   234,\n",
       "          11, 15574,    10,  1248,  1280,     3,   182,     6,   296,\n",
       "         429,    10,    78,    18,   239,    37,    14,    13,    41,\n",
       "           7,     3,    59,     9,    29,  1281,     2,   580,     4,\n",
       "        6750,     5,    95,   380,    10,    19,    18,     4,  1481,\n",
       "           9,    57,   305,  1056,   130,    19,    18,  1528,     3,\n",
       "         288,    21,   315,   609,   776,     6,     4,   107,   624,\n",
       "          47,    53,    10,    19,    82,     2,     4,  1361,   496,\n",
       "          10,    19,   217,    11,     9,   653,   539,  1432,     2,\n",
       "          77,    57,    99, 45175,   160,    19,    59,   390,     9,\n",
       "         445,   248,  1011,   130,    13,  1114,   111,     5,   148,\n",
       "           3,   539,     5,    77,  1612,     6,   257,    46,   521,\n",
       "         712,     5,   539,   362,     7,     4,    77,   767,   103,\n",
       "           3,    59,    42,    31,   834,   460,    35,    26,   103,\n",
       "          15,   659,  1586,  1559,   235,   148,   241,    48,  1618,\n",
       "          31,  1070,     8,    31,   257,   103,   462,    10,    19,\n",
       "          42,     5,  1293,  1583,    23,  1261,   248,    99,  7835,\n",
       "         138,     6,    27,   871,   334,   109, 40782,    24,  2790,\n",
       "           3,    17,   116,   352,    55,    11,  1446,   123,    39,\n",
       "         207,     2,  7478,   654,  7242,     3,   413,    43,     3,\n",
       "        1390,   836,  1517,     2,   430,    90,   936,   702,  7981,\n",
       "           2,  1341,   948,  8002,    88, 13417,   105,    10,    19,\n",
       "         167,   976,     8,    21,    17,    99,    36,     2,   893,\n",
       "           8,   141,    22,    19,   390,    10,   757,     2,   942,\n",
       "          21,   247,   861,    19,   516,    10,     3,   473,     2,\n",
       "          42,    20,   438,    38,    36,  6648,    19,    18,  1051,\n",
       "           3,   166,     2,   202,     2,     3,   228,     4,    85,\n",
       "          10,    98,     3,   558,     2,   584,    21,  1146,   104,\n",
       "       14536,   128,     3,   681,    19,    18,  1120,    25,    21,\n",
       "        1015,     3,  1024,   663,   307,   803,     4,    48,     2,\n",
       "        1059,   931,     2,   428,   120, 13488,   536,    19,   102,\n",
       "         601,   210,     4,   384,     6,     9,   583,    57,   153,\n",
       "          59,  1112,  1119,    31,   989, 16077,    19,    18,  1091,\n",
       "         278,     5,   621,   337,  1042,   490,     2,   536,    13,\n",
       "         472,     2, 16201,   131,    10,    14,    13,   459,     3,\n",
       "          42,     5,   192,     8,     9,   186,   105,  1223,  1218,\n",
       "          23,     4,   144,     6,   414,   110,     2,    16,   287,\n",
       "         426,    37,   158,     5,    33,   213,    19,    18,    51,\n",
       "           3,   937,   505,     5,   381,  1514,   597,     2,     5,\n",
       "         531,   401,   105,   229,    20,    40,     3,   202,     2,\n",
       "         217,    21,   104,     8,    20,    60,    22,   286,     5,\n",
       "         712,   595,     7,   110,   960,    20,    60,     7,   766,\n",
       "         464,     2,  1096,     4,   490,     2,   135,     6,   300,\n",
       "          32,   218,   267,    19,   158,   448,     7,    86,   283,\n",
       "         433,     3,   280,  1062,   244,   148,   954,   256,   150,\n",
       "         265,   223,   175], dtype=int64)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_description = training_set[text_col].sample().iloc[0]\n",
    "print(sample_description)\n",
    "vectorize_layer(sample_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for token in vectorize_layer(sample_description).numpy()[:20]:\n",
    "#     print(f\"{token} ---> \",vectorize_layer.get_vocabulary()[token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Descriptions squashed into 1 average embedding vector, size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=32\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "    GlobalAveragePooling1D(),\n",
    "#     Dropout(0.03),\n",
    "    Dense(1024, activation='elu', name='hidden_layer'),\n",
    "#     Dropout(0.01),\n",
    "#     Dense(2048, activation='elu', name='hidden_layer2'),\n",
    "#     Dropout(0.02),\n",
    "#     Dense(1024, activation='relu', name='hidden_layer2'),\n",
    "    Dense(df_pos.JobTitle.nunique(), name = 'output_layer', activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Text_processing (TextVector  (None, 516)              0         \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 516, 32)           1600000   \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 32)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " hidden_layer (Dense)        (None, 1024)              33792     \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 352)               360800    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,994,592\n",
      "Trainable params: 1,994,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model1, show_dtype=True, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall(),'accuracy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating checkpoints for model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = 'Deep_models_weights'\n",
    "cp_callback = [tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='Deep_model_weights/model.{epoch:02d}-val_loss{val_loss:.3f}-val_precision{val_precision:.3f}-val_recall{val_recall:.3f}.tf', \n",
    "    verbose=1, \n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "\n",
    "# checkpoint_path = 'Deep_models_weights'\n",
    "# callbacks  = [\n",
    "#     keras.callbacks.ModelCheckpoint(\n",
    "#         filepath=checkpoint_path, \n",
    "#         monitor='val_loss',\n",
    "#         verbose=1,\n",
    "#         save_best_only=True,\n",
    "#         save_weights_only=True,\n",
    "#         save_freq='epoch'),\n",
    "#     keras.callbacks.EarlyStopping(\n",
    "#         monitor='val_recall',\n",
    "#         min_delta=0,\n",
    "#         patience=20,\n",
    "#         verbose=1)\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 3.9461 - precision: 0.9968 - recall: 0.0609 - accuracy: 0.1866\n",
      "Epoch 00001: saving model to Deep_model_weights\\model.01-val_loss3.914-val_precision0.991-val_recall0.066.tf\n",
      "20/20 [==============================] - 8s 396ms/step - loss: 3.9461 - precision: 0.9968 - recall: 0.0609 - accuracy: 0.1866 - val_loss: 3.9143 - val_precision: 0.9911 - val_recall: 0.0659 - val_accuracy: 0.1846\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 3.8294 - precision: 0.9962 - recall: 0.0641 - accuracy: 0.1995\n",
      "Epoch 00002: saving model to Deep_model_weights\\model.02-val_loss3.801-val_precision0.992-val_recall0.072.tf\n",
      "20/20 [==============================] - 9s 457ms/step - loss: 3.8294 - precision: 0.9962 - recall: 0.0641 - accuracy: 0.1995 - val_loss: 3.8015 - val_precision: 0.9919 - val_recall: 0.0720 - val_accuracy: 0.2100\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 3.7127 - precision: 0.9938 - recall: 0.0711 - accuracy: 0.2260\n",
      "Epoch 00003: saving model to Deep_model_weights\\model.03-val_loss3.688-val_precision0.992-val_recall0.078.tf\n",
      "20/20 [==============================] - 10s 499ms/step - loss: 3.7127 - precision: 0.9938 - recall: 0.0711 - accuracy: 0.2260 - val_loss: 3.6882 - val_precision: 0.9925 - val_recall: 0.0779 - val_accuracy: 0.2305\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 3.5984 - precision: 0.9857 - recall: 0.0814 - accuracy: 0.2413\n",
      "Epoch 00004: saving model to Deep_model_weights\\model.04-val_loss3.585-val_precision0.969-val_recall0.087.tf\n",
      "20/20 [==============================] - 10s 484ms/step - loss: 3.5984 - precision: 0.9857 - recall: 0.0814 - accuracy: 0.2413 - val_loss: 3.5848 - val_precision: 0.9694 - val_recall: 0.0872 - val_accuracy: 0.2535\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 3.4895 - precision: 0.9433 - recall: 0.0867 - accuracy: 0.2659\n",
      "Epoch 00005: saving model to Deep_model_weights\\model.05-val_loss3.485-val_precision0.961-val_recall0.088.tf\n",
      "20/20 [==============================] - 9s 449ms/step - loss: 3.4895 - precision: 0.9433 - recall: 0.0867 - accuracy: 0.2659 - val_loss: 3.4849 - val_precision: 0.9614 - val_recall: 0.0882 - val_accuracy: 0.2683\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 3.3876 - precision: 0.9286 - recall: 0.0960 - accuracy: 0.2823\n",
      "Epoch 00006: saving model to Deep_model_weights\\model.06-val_loss3.400-val_precision0.916-val_recall0.114.tf\n",
      "20/20 [==============================] - 9s 463ms/step - loss: 3.3876 - precision: 0.9286 - recall: 0.0960 - accuracy: 0.2823 - val_loss: 3.4002 - val_precision: 0.9161 - val_recall: 0.1140 - val_accuracy: 0.2858\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 3.2936 - precision: 0.8949 - recall: 0.1165 - accuracy: 0.3035\n",
      "Epoch 00007: saving model to Deep_model_weights\\model.07-val_loss3.317-val_precision0.924-val_recall0.131.tf\n",
      "20/20 [==============================] - 9s 431ms/step - loss: 3.2936 - precision: 0.8949 - recall: 0.1165 - accuracy: 0.3035 - val_loss: 3.3168 - val_precision: 0.9235 - val_recall: 0.1307 - val_accuracy: 0.2994\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 3.2046 - precision: 0.9111 - recall: 0.1236 - accuracy: 0.3169\n",
      "Epoch 00008: saving model to Deep_model_weights\\model.08-val_loss3.247-val_precision0.882-val_recall0.142.tf\n",
      "20/20 [==============================] - 8s 408ms/step - loss: 3.2046 - precision: 0.9111 - recall: 0.1236 - accuracy: 0.3169 - val_loss: 3.2470 - val_precision: 0.8822 - val_recall: 0.1415 - val_accuracy: 0.3145\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 3.1227 - precision: 0.9100 - recall: 0.1294 - accuracy: 0.3233\n",
      "Epoch 00009: saving model to Deep_model_weights\\model.09-val_loss3.172-val_precision0.919-val_recall0.138.tf\n",
      "20/20 [==============================] - 8s 429ms/step - loss: 3.1227 - precision: 0.9100 - recall: 0.1294 - accuracy: 0.3233 - val_loss: 3.1724 - val_precision: 0.9185 - val_recall: 0.1376 - val_accuracy: 0.3346\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 3.0438 - precision: 0.9008 - recall: 0.1354 - accuracy: 0.3411\n",
      "Epoch 00010: saving model to Deep_model_weights\\model.10-val_loss3.103-val_precision0.923-val_recall0.142.tf\n",
      "20/20 [==============================] - 8s 406ms/step - loss: 3.0438 - precision: 0.9008 - recall: 0.1354 - accuracy: 0.3411 - val_loss: 3.1031 - val_precision: 0.9232 - val_recall: 0.1419 - val_accuracy: 0.3541\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.9629 - precision: 0.9106 - recall: 0.1494 - accuracy: 0.3624\n",
      "Epoch 00011: saving model to Deep_model_weights\\model.11-val_loss3.042-val_precision0.902-val_recall0.175.tf\n",
      "20/20 [==============================] - 10s 483ms/step - loss: 2.9629 - precision: 0.9106 - recall: 0.1494 - accuracy: 0.3624 - val_loss: 3.0423 - val_precision: 0.9023 - val_recall: 0.1746 - val_accuracy: 0.3617\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.8837 - precision: 0.9141 - recall: 0.1749 - accuracy: 0.3794\n",
      "Epoch 00012: saving model to Deep_model_weights\\model.12-val_loss2.974-val_precision0.879-val_recall0.191.tf\n",
      "20/20 [==============================] - 11s 549ms/step - loss: 2.8837 - precision: 0.9141 - recall: 0.1749 - accuracy: 0.3794 - val_loss: 2.9742 - val_precision: 0.8785 - val_recall: 0.1907 - val_accuracy: 0.3720\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.8048 - precision: 0.9014 - recall: 0.1938 - accuracy: 0.3907\n",
      "Epoch 00013: saving model to Deep_model_weights\\model.13-val_loss2.904-val_precision0.893-val_recall0.213.tf\n",
      "20/20 [==============================] - 8s 411ms/step - loss: 2.8048 - precision: 0.9014 - recall: 0.1938 - accuracy: 0.3907 - val_loss: 2.9040 - val_precision: 0.8927 - val_recall: 0.2130 - val_accuracy: 0.3881\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.7281 - precision: 0.8965 - recall: 0.2237 - accuracy: 0.4036\n",
      "Epoch 00014: saving model to Deep_model_weights\\model.14-val_loss2.844-val_precision0.889-val_recall0.237.tf\n",
      "20/20 [==============================] - 9s 461ms/step - loss: 2.7281 - precision: 0.8965 - recall: 0.2237 - accuracy: 0.4036 - val_loss: 2.8436 - val_precision: 0.8892 - val_recall: 0.2370 - val_accuracy: 0.4013\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.6539 - precision: 0.8806 - recall: 0.2489 - accuracy: 0.4206\n",
      "Epoch 00015: saving model to Deep_model_weights\\model.15-val_loss2.777-val_precision0.858-val_recall0.256.tf\n",
      "20/20 [==============================] - 8s 409ms/step - loss: 2.6539 - precision: 0.8806 - recall: 0.2489 - accuracy: 0.4206 - val_loss: 2.7770 - val_precision: 0.8583 - val_recall: 0.2562 - val_accuracy: 0.4206\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.5811 - precision: 0.8788 - recall: 0.2669 - accuracy: 0.4385\n",
      "Epoch 00016: saving model to Deep_model_weights\\model.16-val_loss2.714-val_precision0.869-val_recall0.269.tf\n",
      "20/20 [==============================] - 9s 454ms/step - loss: 2.5811 - precision: 0.8788 - recall: 0.2669 - accuracy: 0.4385 - val_loss: 2.7144 - val_precision: 0.8689 - val_recall: 0.2686 - val_accuracy: 0.4344\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.5105 - precision: 0.8769 - recall: 0.2773 - accuracy: 0.4538\n",
      "Epoch 00017: saving model to Deep_model_weights\\model.17-val_loss2.659-val_precision0.852-val_recall0.276.tf\n",
      "20/20 [==============================] - 8s 407ms/step - loss: 2.5105 - precision: 0.8769 - recall: 0.2773 - accuracy: 0.4538 - val_loss: 2.6591 - val_precision: 0.8518 - val_recall: 0.2759 - val_accuracy: 0.4361\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.4422 - precision: 0.8751 - recall: 0.2850 - accuracy: 0.4653\n",
      "Epoch 00018: saving model to Deep_model_weights\\model.18-val_loss2.603-val_precision0.860-val_recall0.291.tf\n",
      "20/20 [==============================] - 9s 449ms/step - loss: 2.4422 - precision: 0.8751 - recall: 0.2850 - accuracy: 0.4653 - val_loss: 2.6032 - val_precision: 0.8605 - val_recall: 0.2913 - val_accuracy: 0.4485\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.3803 - precision: 0.8714 - recall: 0.3023 - accuracy: 0.4744\n",
      "Epoch 00019: saving model to Deep_model_weights\\model.19-val_loss2.548-val_precision0.862-val_recall0.304.tf\n",
      "20/20 [==============================] - 8s 414ms/step - loss: 2.3803 - precision: 0.8714 - recall: 0.3023 - accuracy: 0.4744 - val_loss: 2.5482 - val_precision: 0.8625 - val_recall: 0.3037 - val_accuracy: 0.4570\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.3188 - precision: 0.8808 - recall: 0.3208 - accuracy: 0.4850\n",
      "Epoch 00020: saving model to Deep_model_weights\\model.20-val_loss2.500-val_precision0.859-val_recall0.312.tf\n",
      "20/20 [==============================] - 9s 440ms/step - loss: 2.3188 - precision: 0.8808 - recall: 0.3208 - accuracy: 0.4850 - val_loss: 2.5003 - val_precision: 0.8585 - val_recall: 0.3117 - val_accuracy: 0.4731\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.2622 - precision: 0.8835 - recall: 0.3352 - accuracy: 0.5001\n",
      "Epoch 00021: saving model to Deep_model_weights\\model.21-val_loss2.462-val_precision0.857-val_recall0.327.tf\n",
      "20/20 [==============================] - 8s 418ms/step - loss: 2.2622 - precision: 0.8835 - recall: 0.3352 - accuracy: 0.5001 - val_loss: 2.4620 - val_precision: 0.8566 - val_recall: 0.3267 - val_accuracy: 0.4834\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.2065 - precision: 0.8873 - recall: 0.3440 - accuracy: 0.5089\n",
      "Epoch 00022: saving model to Deep_model_weights\\model.22-val_loss2.416-val_precision0.861-val_recall0.337.tf\n",
      "20/20 [==============================] - 8s 425ms/step - loss: 2.2065 - precision: 0.8873 - recall: 0.3440 - accuracy: 0.5089 - val_loss: 2.4164 - val_precision: 0.8613 - val_recall: 0.3373 - val_accuracy: 0.4893\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.1558 - precision: 0.8864 - recall: 0.3567 - accuracy: 0.5197\n",
      "Epoch 00023: saving model to Deep_model_weights\\model.23-val_loss2.377-val_precision0.866-val_recall0.342.tf\n",
      "20/20 [==============================] - 8s 424ms/step - loss: 2.1558 - precision: 0.8864 - recall: 0.3567 - accuracy: 0.5197 - val_loss: 2.3767 - val_precision: 0.8655 - val_recall: 0.3421 - val_accuracy: 0.5005\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.1065 - precision: 0.8890 - recall: 0.3670 - accuracy: 0.5283\n",
      "Epoch 00024: saving model to Deep_model_weights\\model.24-val_loss2.334-val_precision0.862-val_recall0.362.tf\n",
      "20/20 [==============================] - 8s 415ms/step - loss: 2.1065 - precision: 0.8890 - recall: 0.3670 - accuracy: 0.5283 - val_loss: 2.3343 - val_precision: 0.8617 - val_recall: 0.3617 - val_accuracy: 0.5023\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.0573 - precision: 0.8958 - recall: 0.3796 - accuracy: 0.5363\n",
      "Epoch 00025: saving model to Deep_model_weights\\model.25-val_loss2.301-val_precision0.865-val_recall0.368.tf\n",
      "20/20 [==============================] - 9s 442ms/step - loss: 2.0573 - precision: 0.8958 - recall: 0.3796 - accuracy: 0.5363 - val_loss: 2.3014 - val_precision: 0.8654 - val_recall: 0.3682 - val_accuracy: 0.5092\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.0144 - precision: 0.8938 - recall: 0.3903 - accuracy: 0.5470\n",
      "Epoch 00026: saving model to Deep_model_weights\\model.26-val_loss2.274-val_precision0.869-val_recall0.383.tf\n",
      "20/20 [==============================] - 9s 438ms/step - loss: 2.0144 - precision: 0.8938 - recall: 0.3903 - accuracy: 0.5470 - val_loss: 2.2740 - val_precision: 0.8687 - val_recall: 0.3828 - val_accuracy: 0.5188\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.9705 - precision: 0.8984 - recall: 0.4013 - accuracy: 0.5531\n",
      "Epoch 00027: saving model to Deep_model_weights\\model.27-val_loss2.236-val_precision0.874-val_recall0.387.tf\n",
      "20/20 [==============================] - 9s 438ms/step - loss: 1.9705 - precision: 0.8984 - recall: 0.4013 - accuracy: 0.5531 - val_loss: 2.2362 - val_precision: 0.8739 - val_recall: 0.3873 - val_accuracy: 0.5267\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.9309 - precision: 0.9018 - recall: 0.4102 - accuracy: 0.5646\n",
      "Epoch 00028: saving model to Deep_model_weights\\model.28-val_loss2.213-val_precision0.871-val_recall0.392.tf\n",
      "20/20 [==============================] - 8s 416ms/step - loss: 1.9309 - precision: 0.9018 - recall: 0.4102 - accuracy: 0.5646 - val_loss: 2.2135 - val_precision: 0.8714 - val_recall: 0.3920 - val_accuracy: 0.5340\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.8956 - precision: 0.8995 - recall: 0.4151 - accuracy: 0.5683\n",
      "Epoch 00029: saving model to Deep_model_weights\\model.29-val_loss2.195-val_precision0.871-val_recall0.395.tf\n",
      "20/20 [==============================] - 9s 435ms/step - loss: 1.8956 - precision: 0.8995 - recall: 0.4151 - accuracy: 0.5683 - val_loss: 2.1948 - val_precision: 0.8711 - val_recall: 0.3952 - val_accuracy: 0.5381\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.8572 - precision: 0.9040 - recall: 0.4218 - accuracy: 0.5773\n",
      "Epoch 00030: saving model to Deep_model_weights\\model.30-val_loss2.164-val_precision0.873-val_recall0.399.tf\n",
      "20/20 [==============================] - 8s 412ms/step - loss: 1.8572 - precision: 0.9040 - recall: 0.4218 - accuracy: 0.5773 - val_loss: 2.1643 - val_precision: 0.8728 - val_recall: 0.3985 - val_accuracy: 0.5412\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.8192 - precision: 0.9092 - recall: 0.4287 - accuracy: 0.5860\n",
      "Epoch 00031: saving model to Deep_model_weights\\model.31-val_loss2.136-val_precision0.879-val_recall0.403.tf\n",
      "20/20 [==============================] - 9s 441ms/step - loss: 1.8192 - precision: 0.9092 - recall: 0.4287 - accuracy: 0.5860 - val_loss: 2.1358 - val_precision: 0.8794 - val_recall: 0.4033 - val_accuracy: 0.5528\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.7860 - precision: 0.9120 - recall: 0.4348 - accuracy: 0.5905\n",
      "Epoch 00032: saving model to Deep_model_weights\\model.32-val_loss2.120-val_precision0.872-val_recall0.410.tf\n",
      "20/20 [==============================] - 8s 419ms/step - loss: 1.7860 - precision: 0.9120 - recall: 0.4348 - accuracy: 0.5905 - val_loss: 2.1197 - val_precision: 0.8720 - val_recall: 0.4104 - val_accuracy: 0.5521\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.7556 - precision: 0.9164 - recall: 0.4421 - accuracy: 0.6010\n",
      "Epoch 00033: saving model to Deep_model_weights\\model.33-val_loss2.095-val_precision0.873-val_recall0.422.tf\n",
      "20/20 [==============================] - 9s 437ms/step - loss: 1.7556 - precision: 0.9164 - recall: 0.4421 - accuracy: 0.6010 - val_loss: 2.0947 - val_precision: 0.8734 - val_recall: 0.4224 - val_accuracy: 0.5566\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.7243 - precision: 0.9120 - recall: 0.4517 - accuracy: 0.6043\n",
      "Epoch 00034: saving model to Deep_model_weights\\model.34-val_loss2.081-val_precision0.870-val_recall0.422.tf\n",
      "20/20 [==============================] - 8s 411ms/step - loss: 1.7243 - precision: 0.9120 - recall: 0.4517 - accuracy: 0.6043 - val_loss: 2.0810 - val_precision: 0.8704 - val_recall: 0.4218 - val_accuracy: 0.5582\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.6932 - precision: 0.9187 - recall: 0.4557 - accuracy: 0.6115\n",
      "Epoch 00035: saving model to Deep_model_weights\\model.35-val_loss2.056-val_precision0.884-val_recall0.427.tf\n",
      "20/20 [==============================] - 9s 430ms/step - loss: 1.6932 - precision: 0.9187 - recall: 0.4557 - accuracy: 0.6115 - val_loss: 2.0564 - val_precision: 0.8835 - val_recall: 0.4269 - val_accuracy: 0.5721\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.6608 - precision: 0.9204 - recall: 0.4627 - accuracy: 0.6200\n",
      "Epoch 00036: saving model to Deep_model_weights\\model.36-val_loss2.037-val_precision0.878-val_recall0.437.tf\n",
      "20/20 [==============================] - 8s 411ms/step - loss: 1.6608 - precision: 0.9204 - recall: 0.4627 - accuracy: 0.6200 - val_loss: 2.0369 - val_precision: 0.8779 - val_recall: 0.4371 - val_accuracy: 0.5763\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.6320 - precision: 0.9227 - recall: 0.4730 - accuracy: 0.6243\n",
      "Epoch 00037: saving model to Deep_model_weights\\model.37-val_loss2.021-val_precision0.879-val_recall0.442.tf\n",
      "20/20 [==============================] - 9s 440ms/step - loss: 1.6320 - precision: 0.9227 - recall: 0.4730 - accuracy: 0.6243 - val_loss: 2.0207 - val_precision: 0.8788 - val_recall: 0.4422 - val_accuracy: 0.5796\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.6036 - precision: 0.9233 - recall: 0.4781 - accuracy: 0.6313\n",
      "Epoch 00038: saving model to Deep_model_weights\\model.38-val_loss2.012-val_precision0.877-val_recall0.439.tf\n",
      "20/20 [==============================] - 10s 509ms/step - loss: 1.6036 - precision: 0.9233 - recall: 0.4781 - accuracy: 0.6313 - val_loss: 2.0121 - val_precision: 0.8772 - val_recall: 0.4387 - val_accuracy: 0.5786\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.5775 - precision: 0.9229 - recall: 0.4854 - accuracy: 0.6352\n",
      "Epoch 00039: saving model to Deep_model_weights\\model.39-val_loss1.992-val_precision0.877-val_recall0.450.tf\n",
      "20/20 [==============================] - 9s 453ms/step - loss: 1.5775 - precision: 0.9229 - recall: 0.4854 - accuracy: 0.6352 - val_loss: 1.9922 - val_precision: 0.8766 - val_recall: 0.4503 - val_accuracy: 0.5828\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.5494 - precision: 0.9256 - recall: 0.4932 - accuracy: 0.6440\n",
      "Epoch 00040: saving model to Deep_model_weights\\model.40-val_loss1.979-val_precision0.880-val_recall0.460.tf\n",
      "20/20 [==============================] - 8s 427ms/step - loss: 1.5494 - precision: 0.9256 - recall: 0.4932 - accuracy: 0.6440 - val_loss: 1.9792 - val_precision: 0.8802 - val_recall: 0.4598 - val_accuracy: 0.5828\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.5228 - precision: 0.9275 - recall: 0.5002 - accuracy: 0.6489\n",
      "Epoch 00041: saving model to Deep_model_weights\\model.41-val_loss1.964-val_precision0.885-val_recall0.457.tf\n",
      "20/20 [==============================] - 8s 402ms/step - loss: 1.5228 - precision: 0.9275 - recall: 0.5002 - accuracy: 0.6489 - val_loss: 1.9644 - val_precision: 0.8853 - val_recall: 0.4572 - val_accuracy: 0.5967\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.4983 - precision: 0.9296 - recall: 0.5064 - accuracy: 0.6566\n",
      "Epoch 00042: saving model to Deep_model_weights\\model.42-val_loss1.950-val_precision0.877-val_recall0.463.tf\n",
      "20/20 [==============================] - 9s 430ms/step - loss: 1.4983 - precision: 0.9296 - recall: 0.5064 - accuracy: 0.6566 - val_loss: 1.9496 - val_precision: 0.8770 - val_recall: 0.4633 - val_accuracy: 0.5940\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.4751 - precision: 0.9312 - recall: 0.5152 - accuracy: 0.6613\n",
      "Epoch 00043: saving model to Deep_model_weights\\model.43-val_loss1.948-val_precision0.885-val_recall0.468.tf\n",
      "20/20 [==============================] - 8s 415ms/step - loss: 1.4751 - precision: 0.9312 - recall: 0.5152 - accuracy: 0.6613 - val_loss: 1.9480 - val_precision: 0.8850 - val_recall: 0.4680 - val_accuracy: 0.6068\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.4517 - precision: 0.9367 - recall: 0.5208 - accuracy: 0.6744\n",
      "Epoch 00044: saving model to Deep_model_weights\\model.44-val_loss1.925-val_precision0.886-val_recall0.473.tf\n",
      "20/20 [==============================] - 9s 432ms/step - loss: 1.4517 - precision: 0.9367 - recall: 0.5208 - accuracy: 0.6744 - val_loss: 1.9250 - val_precision: 0.8857 - val_recall: 0.4729 - val_accuracy: 0.6107\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.4252 - precision: 0.9373 - recall: 0.5284 - accuracy: 0.6753\n",
      "Epoch 00045: saving model to Deep_model_weights\\model.45-val_loss1.914-val_precision0.884-val_recall0.485.tf\n",
      "20/20 [==============================] - 8s 405ms/step - loss: 1.4252 - precision: 0.9373 - recall: 0.5284 - accuracy: 0.6753 - val_loss: 1.9144 - val_precision: 0.8843 - val_recall: 0.4846 - val_accuracy: 0.6093\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.4028 - precision: 0.9358 - recall: 0.5380 - accuracy: 0.6831\n",
      "Epoch 00046: saving model to Deep_model_weights\\model.46-val_loss1.906-val_precision0.886-val_recall0.491.tf\n",
      "20/20 [==============================] - 9s 436ms/step - loss: 1.4028 - precision: 0.9358 - recall: 0.5380 - accuracy: 0.6831 - val_loss: 1.9060 - val_precision: 0.8856 - val_recall: 0.4907 - val_accuracy: 0.6125\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.3823 - precision: 0.9398 - recall: 0.5446 - accuracy: 0.6888\n",
      "Epoch 00047: saving model to Deep_model_weights\\model.47-val_loss1.892-val_precision0.888-val_recall0.491.tf\n",
      "20/20 [==============================] - 8s 419ms/step - loss: 1.3823 - precision: 0.9398 - recall: 0.5446 - accuracy: 0.6888 - val_loss: 1.8920 - val_precision: 0.8881 - val_recall: 0.4907 - val_accuracy: 0.6205\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.3575 - precision: 0.9410 - recall: 0.5490 - accuracy: 0.6946\n",
      "Epoch 00048: saving model to Deep_model_weights\\model.48-val_loss1.886-val_precision0.884-val_recall0.497.tf\n",
      "20/20 [==============================] - 9s 433ms/step - loss: 1.3575 - precision: 0.9410 - recall: 0.5490 - accuracy: 0.6946 - val_loss: 1.8858 - val_precision: 0.8842 - val_recall: 0.4973 - val_accuracy: 0.6221\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.3372 - precision: 0.9431 - recall: 0.5571 - accuracy: 0.6999\n",
      "Epoch 00049: saving model to Deep_model_weights\\model.49-val_loss1.878-val_precision0.883-val_recall0.501.tf\n",
      "20/20 [==============================] - 8s 413ms/step - loss: 1.3372 - precision: 0.9431 - recall: 0.5571 - accuracy: 0.6999 - val_loss: 1.8783 - val_precision: 0.8825 - val_recall: 0.5013 - val_accuracy: 0.6259\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - ETA: 0s - loss: 1.3180 - precision: 0.9440 - recall: 0.5606 - accuracy: 0.7043\n",
      "Epoch 00050: saving model to Deep_model_weights\\model.50-val_loss1.869-val_precision0.888-val_recall0.504.tf\n",
      "20/20 [==============================] - 9s 439ms/step - loss: 1.3180 - precision: 0.9440 - recall: 0.5606 - accuracy: 0.7043 - val_loss: 1.8693 - val_precision: 0.8880 - val_recall: 0.5038 - val_accuracy: 0.6314\n",
      "Wall time: 7min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(\n",
    "    training_set[text_col],\n",
    "    train__y_labels,\n",
    "    epochs=50,\n",
    "    batch_size=1024,\n",
    "    verbose=1,\n",
    "    callbacks=cp_callback,\n",
    "    validation_data = (test_set[text_col], test__y_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### !!!! ADD OR DELETE - Descriptions words concatinated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=32\n",
    "\n",
    "model2 = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "#     GlobalAveragePooling1D(),\n",
    "    Reshape((embedding_dim * sequence_length, ), name='concat_words'),\n",
    "#     Dropout(0.1),\n",
    "    Dense(4096, activation='relu', name='hidden_layer_1'),\n",
    "#     Dropout(0.04),\n",
    "#     Dense(2048, activation='relu', name='hidden_layer_2'),\n",
    "    Dense(df_pos.JobTitle.nunique(), name = 'output_layer')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model2, show_dtype=True, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model2.fit(\n",
    "    training_set[text_col],\n",
    "    train__y_labels,\n",
    "    epochs=10,\n",
    "    batch_size=1024,\n",
    "    verbose=1,    \n",
    "    validation_data = (test_set[text_col], test__y_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['token_with_best_prediction'] = model.predict(test_set[text_col]).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['prob_token_with_best_prediction'] = model.predict(test_set[text_col]).max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_set.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### !!!! ADD OR DELETE - Adding additional features (besides text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Extracting Year column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "def extract_year_from_title(title):\n",
    "    try:\n",
    "        year = parse(title, fuzzy=True).year\n",
    "        return str(int(year)) if year > 1800 else None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_title = wine_reviews.sample().title.iloc[0]\n",
    "print(f'Title is: {sample_title}. Extracted year: {extract_year_from_title(sample_title)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews['year'] = wine_reviews.title.apply(extract_year_from_title)\n",
    "wine_reviews['year'].value_counts(dropna=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the year input informative? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.groupby('year').points.describe().query('count > 20').sort_values(by='mean',ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews = wine_reviews.reset_index() # To ensure correctness with the below join operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_tokens = vectorize_layer(wine_reviews[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_cols = [f'w_{i}' for i in range(1, description_tokens.shape[1] + 1)]\n",
    "features_df = pd.DataFrame(description_tokens.numpy(), columns=description_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df.join(wine_reviews[['points','price','country','year','variety','province']])\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df[categorical_featurs] = features_df[categorical_featurs].fillna('Unknown')\n",
    "features_df.price = features_df.price.fillna(features_df.price.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.country = pd.factorize(features_df.country)[0]\n",
    "features_df.year = pd.factorize(features_df.year)[0]\n",
    "features_df.variety = pd.factorize(features_df.variety)[0]\n",
    "features_df.province = pd.factorize(features_df.province)[0]\n",
    "features_df.year = pd.factorize(features_df.year)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df[categorical_featurs].apply(lambda x: pd.Series({'nunique': x.nunique(),\n",
    "                                                            'max': x.max(),\n",
    "                                                            'min': x.min()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import layers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_input = Input(\n",
    "    shape=(sequence_length,), dtype='int64', name='description'\n",
    ")\n",
    "\n",
    "year_input = Input(\n",
    "    shape=(1,), name=\"year\", dtype='int64'\n",
    ")  \n",
    "\n",
    "country_input = Input(\n",
    "    shape=(1,), name=\"country\", dtype='int64'\n",
    ")  \n",
    "\n",
    "province_input = Input(\n",
    "    shape=(1,), name=\"province\", dtype='int64'\n",
    ")\n",
    "\n",
    "variety_input = Input(\n",
    "    shape=(1,), name=\"variety\", dtype='int64'\n",
    ")\n",
    "\n",
    "price_input = Input(\n",
    "    shape=(1,), name=\"price\",\n",
    ")\n",
    "\n",
    "word_features = layers.Embedding(vocab_size, embedding_dim, input_length=sequence_length, name='word_embeddings')(description_input)\n",
    "word_features = layers.Reshape((embedding_dim * sequence_length,), name='concat_words')(word_features)\n",
    "\n",
    "year_features = layers.Embedding(100, 3, name='year_embeddings')(year_input)\n",
    "year_features = layers.Reshape((3,), name='concat_year')(year_features)\n",
    "\n",
    "country_features = layers.Embedding(50, 2, name='country_embeddings')(country_input)\n",
    "country_features = layers.Reshape((2,), name='concat_country')(country_features)\n",
    "\n",
    "province_features = layers.Embedding(500, 5, name='province_embeddings')(province_input)\n",
    "province_features = layers.Reshape((5,), name='concat_province')(province_features)\n",
    "\n",
    "variety_features = layers.Embedding(1000, 4, name='variety_embeddings')(variety_input)\n",
    "variety_features = layers.Reshape((4,), name='concat_variety')(variety_features)\n",
    "\n",
    "# Merge all available features into a single large vector via concatenation\n",
    "feature_vector = layers.concatenate([word_features, year_features, country_features, province_features, variety_features, price_input])\n",
    "x = layers.Dropout(0.2)(feature_vector)\n",
    "x = layers.Dense(256, activation='relu', name='Hidden')(x)\n",
    "# Outputs:\n",
    "predictions = layers.Dense(1, name=\"output\")(x)\n",
    "\n",
    "# Instantiate an end-to-end model predicting E,I,O:\n",
    "model = Model(\n",
    "    inputs=[description_input, year_input, country_input, province_input, variety_input, price_input],\n",
    "    outputs=predictions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_dtype=True, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = features_df.sample(frac=0.8, random_state=42)\n",
    "test_set = features_df[~features_df.index.isin(training_set.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(training_set) + len(test_set) == len(wine_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(\n",
    "    {\"description\": training_set[description_cols].values, \n",
    "     \"year\": training_set['year'].values,\n",
    "     \"country\": training_set['country'].values,\n",
    "     \"province\": training_set['province'].values,\n",
    "     \"variety\": training_set['variety'].values, \n",
    "     'price': training_set['price'].values},\n",
    "    \n",
    "    {\"output\": training_set['points'].values},\n",
    "    validation_data=([test_set[description_cols].values, \n",
    "                      test_set['year'].values, \n",
    "                      test_set['country'].values, \n",
    "                      test_set['province'].values, \n",
    "                      test_set['variety'].values, \n",
    "                      test_set['price'].values],\n",
    "                     test_set['points'].values),\n",
    "    epochs=10,\n",
    "    batch_size=512,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['dnn_prediction'] = model.predict({'description': test_set[description_cols], \n",
    "                                            'year': test_set['year'], \n",
    "                                            'country': test_set['country'], \n",
    "                                            'province': test_set['province'], \n",
    "                                            'variety': test_set['variety'], \n",
    "                                            'price': test_set['price']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_prediction_quality(test_set, 'dnn_prediction', target_col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
