{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job title prediction with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys; sys.path.append('../')                                                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.4.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cufflinks as cf; cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization, Embedding, Dense, GlobalAveragePooling1D, Dropout, Reshape, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExtJobTitleText</th>\n",
       "      <th>JobTitle</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25278</th>\n",
       "      <td>Seasonal Warehouse Associate - Milton</td>\n",
       "      <td>Warehouse Worker</td>\n",
       "      <td>NOW OFFERING A $500 SIGN-ON BONUS!!EARN UP TO $19.50 ON NIGHT SHIFTSAttend one of our upcoming walk-in hiring events! No appointment necessary!Walk-ins welcome—or apply online and then schedule an appointment that works for you. Hiring EventDate/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5179</th>\n",
       "      <td>Assembler - Second Shift</td>\n",
       "      <td>Assembler</td>\n",
       "      <td>Assemblers Needed – Entry Level - Day Shift –must be able to start at 5am - needed for a long term contract opportunity with our client located in Tempe, AZ What You Will Be Doing:Tube Assembly processors perform day to day production activities ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             ExtJobTitleText          JobTitle  \\\n",
       "25278  Seasonal Warehouse Associate - Milton  Warehouse Worker   \n",
       "5179                Assembler - Second Shift         Assembler   \n",
       "\n",
       "                                                                                                                                                                                                                                                     Description  \n",
       "25278  NOW OFFERING A $500 SIGN-ON BONUS!!EARN UP TO $19.50 ON NIGHT SHIFTSAttend one of our upcoming walk-in hiring events! No appointment necessary!Walk-ins welcome—or apply online and then schedule an appointment that works for you. Hiring EventDate/...  \n",
       "5179   Assemblers Needed – Entry Level - Day Shift –must be able to start at 5am - needed for a long term contract opportunity with our client located in Tempe, AZ What You Will Be Doing:Tube Assembly processors perform day to day production activities ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos = pd.read_csv('datasets/data_clean.csv')\n",
    "df_pos.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25405, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos[\"JobTitle_tokenized\"] = pd.factorize(df_pos.JobTitle)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pos[\"ext_job_title_tokenized\"] = pd.factorize(df_pos.ExtJobTitleText)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Warehouse Worker</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Packager</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pediatric Speech Language Pathologist</th>\n",
       "      <td>958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Retail Sales Representative</th>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Registered Nurse (RN)</th>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maintenance Planner</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hospital Admissions Coordinator</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Finance Manager</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saw Operator</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sewing Machine Operator</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>352 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       count\n",
       "Warehouse Worker                        1000\n",
       "Packager                                1000\n",
       "Pediatric Speech Language Pathologist    958\n",
       "Retail Sales Representative              810\n",
       "Registered Nurse (RN)                    701\n",
       "...                                      ...\n",
       "Maintenance Planner                       10\n",
       "Hospital Admissions Coordinator           10\n",
       "Finance Manager                           10\n",
       "Saw Operator                              10\n",
       "Sewing Machine Operator                   10\n",
       "\n",
       "[352 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos.JobTitle.value_counts().to_frame(name='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train and test set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = df_pos[df_pos['Description'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col, target_col = 'Description', 'JobTitle'\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "label_as_binary = LabelBinarizer()\n",
    "\n",
    "label_as_binary.fit(df_pos[target_col])\n",
    "\n",
    "training_set = df_pos[[text_col, target_col]].sample(frac=0.8, random_state=41)\n",
    "test_set = df_pos[~df_pos.index.isin(training_set.index)][[text_col, target_col]]\n",
    "\n",
    "train__y_labels = label_as_binary.transform(training_set[target_col])\n",
    "test__y_labels = label_as_binary.transform(test_set[target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(training_set) + len(test_set) == len(df_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a good size for the sequence_length? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50    225.00\n",
       "0.60    285.00\n",
       "0.70    334.00\n",
       "0.80    386.00\n",
       "0.90    516.00\n",
       "0.95    606.85\n",
       "0.99    940.00\n",
       "Name: Description, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos.Description.apply(lambda x: len(x.split(' '))).quantile([0.5,0.6,0.7,0.8,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a good size for the vocabulary? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16663\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_transformer = CountVectorizer(min_df=5).fit(df_pos['Description'])\n",
    "\n",
    "# Print total number of vocab words\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 16663\n",
    "sequence_length = 516\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to integers. Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    #standardize=lambda text: tf.strings.lower(text), # You can use your own normalization function here\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    name = 'Text_processing',\n",
    "    output_sequence_length=sequence_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(training_set[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you a Senior Proposal Writer who enjoys the challenge of writing proposals and working on a collaborative team? Are you looking for an opportunity to work with an established company that values its employee’s enthusiasm and technical contributions? If so, we want to talk to you! Our client has an exciting remote (IN THE USA) contract to hire opportunity for a Senior Proposal Writer! The ideal candidate is seeking challenging work and\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(516,), dtype=int64, numpy=\n",
       "array([  13,   10,    5, 1270, 1800, 4035,   37, 2644,    4, 2531,    6,\n",
       "       1643, 3457,    2,   83,   23,    5,  674,   29,   13,   10,   41,\n",
       "          7,   20,   40,    3,   17,   11,   20, 1025,   63,   22,  585,\n",
       "        414, 5352, 7513,    2,  572,  541,   48,  115,   14,  515,    3,\n",
       "       2520,    3,   10,    9,  293,  154,   20, 1935, 1653,    8,    4,\n",
       "       4343,  419,    3,  240,   40,    7,    5, 1270, 1800, 4035,    4,\n",
       "        709,  765,   12,  206, 2934,   17,    2,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "      dtype=int64)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_description = training_set[text_col].sample().iloc[0]\n",
    "print(sample_description)\n",
    "vectorize_layer(sample_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for token in vectorize_layer(sample_description).numpy()[:20]:\n",
    "#     print(f\"{token} ---> \",vectorize_layer.get_vocabulary()[token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Descriptions squashed into 1 average embedding vector, size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim1=32\n",
    "\n",
    "model1 = tf.keras.Sequential([\n",
    "    vectorize_layer1,\n",
    "    Embedding(vocab_size1, embedding_dim1, name=\"embedding\"),\n",
    "    GlobalAveragePooling1D(),\n",
    "#     Dropout(0.03),\n",
    "    Dense(4096, activation='elu', name='hidden_layer'),\n",
    "#     Dropout(0.01),\n",
    "#     Dense(2048, activation='elu', name='hidden_layer2'),\n",
    "#     Dropout(0.02),\n",
    "#     Dense(1024, activation='relu', name='hidden_layer2'),\n",
    "    Dense(df_pos1.JobTitle.nunique(), name = 'output_layer', activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model1, show_dtype=True, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating checkpoints for model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = 'Deep_models_weights'\n",
    "cp_callback = [tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='Deep_model_weights/model.{epoch:02d}-val_loss{val_loss:.3f}-val_precision{val_precision:.3f}-val_recall{val_recall:.3f}.tf', \n",
    "    verbose=1, \n",
    "    save_weights_only=True,\n",
    "    save_freq= 'epoch')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "\n",
    "# checkpoint_path = 'Deep_models_weights'\n",
    "# callbacks  = [\n",
    "#     keras.callbacks.ModelCheckpoint(\n",
    "#         filepath=checkpoint_path, \n",
    "#         monitor='val_loss',\n",
    "#         verbose=1,\n",
    "#         save_best_only=True,\n",
    "#         save_weights_only=True,\n",
    "#         save_freq='epoch'),\n",
    "#     keras.callbacks.EarlyStopping(\n",
    "#         monitor='val_recall',\n",
    "#         min_delta=0,\n",
    "#         patience=20,\n",
    "#         verbose=1)\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(\n",
    "    training_set[text_col],\n",
    "    train__y_labels,\n",
    "    epochs=10,\n",
    "    batch_size=1024,\n",
    "    verbose=1,\n",
    "    callbacks=cp_callback,\n",
    "    validation_data = (test_set[text_col], test__y_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### !!!! ADD OR DELETE - Descriptions words concatinated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=32\n",
    "\n",
    "model2 = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "#     GlobalAveragePooling1D(),\n",
    "    Reshape((embedding_dim * sequence_length, ), name='concat_words'),\n",
    "#     Dropout(0.1),\n",
    "    Dense(4096, activation='relu', name='hidden_layer_1'),\n",
    "#     Dropout(0.04),\n",
    "#     Dense(2048, activation='relu', name='hidden_layer_2'),\n",
    "    Dense(df_pos.JobTitle.nunique(), name = 'output_layer')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model2, show_dtype=True, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model2.fit(\n",
    "    training_set[text_col],\n",
    "    train__y_labels,\n",
    "    epochs=10,\n",
    "    batch_size=1024,\n",
    "    verbose=1,    \n",
    "    validation_data = (test_set[text_col], test__y_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['token_with_best_prediction'] = model.predict(test_set[text_col]).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['prob_token_with_best_prediction'] = model.predict(test_set[text_col]).max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_set.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### !!!! ADD OR DELETE - Adding additional features (besides text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Extracting Year column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "def extract_year_from_title(title):\n",
    "    try:\n",
    "        year = parse(title, fuzzy=True).year\n",
    "        return str(int(year)) if year > 1800 else None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_title = wine_reviews.sample().title.iloc[0]\n",
    "print(f'Title is: {sample_title}. Extracted year: {extract_year_from_title(sample_title)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews['year'] = wine_reviews.title.apply(extract_year_from_title)\n",
    "wine_reviews['year'].value_counts(dropna=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the year input informative? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.groupby('year').points.describe().query('count > 20').sort_values(by='mean',ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews = wine_reviews.reset_index() # To ensure correctness with the below join operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_tokens = vectorize_layer(wine_reviews[text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_cols = [f'w_{i}' for i in range(1, description_tokens.shape[1] + 1)]\n",
    "features_df = pd.DataFrame(description_tokens.numpy(), columns=description_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df.join(wine_reviews[['points','price','country','year','variety','province']])\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df[categorical_featurs] = features_df[categorical_featurs].fillna('Unknown')\n",
    "features_df.price = features_df.price.fillna(features_df.price.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.country = pd.factorize(features_df.country)[0]\n",
    "features_df.year = pd.factorize(features_df.year)[0]\n",
    "features_df.variety = pd.factorize(features_df.variety)[0]\n",
    "features_df.province = pd.factorize(features_df.province)[0]\n",
    "features_df.year = pd.factorize(features_df.year)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df[categorical_featurs].apply(lambda x: pd.Series({'nunique': x.nunique(),\n",
    "                                                            'max': x.max(),\n",
    "                                                            'min': x.min()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import layers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_input = Input(\n",
    "    shape=(sequence_length,), dtype='int64', name='description'\n",
    ")\n",
    "\n",
    "year_input = Input(\n",
    "    shape=(1,), name=\"year\", dtype='int64'\n",
    ")  \n",
    "\n",
    "country_input = Input(\n",
    "    shape=(1,), name=\"country\", dtype='int64'\n",
    ")  \n",
    "\n",
    "province_input = Input(\n",
    "    shape=(1,), name=\"province\", dtype='int64'\n",
    ")\n",
    "\n",
    "variety_input = Input(\n",
    "    shape=(1,), name=\"variety\", dtype='int64'\n",
    ")\n",
    "\n",
    "price_input = Input(\n",
    "    shape=(1,), name=\"price\",\n",
    ")\n",
    "\n",
    "word_features = layers.Embedding(vocab_size, embedding_dim, input_length=sequence_length, name='word_embeddings')(description_input)\n",
    "word_features = layers.Reshape((embedding_dim * sequence_length,), name='concat_words')(word_features)\n",
    "\n",
    "year_features = layers.Embedding(100, 3, name='year_embeddings')(year_input)\n",
    "year_features = layers.Reshape((3,), name='concat_year')(year_features)\n",
    "\n",
    "country_features = layers.Embedding(50, 2, name='country_embeddings')(country_input)\n",
    "country_features = layers.Reshape((2,), name='concat_country')(country_features)\n",
    "\n",
    "province_features = layers.Embedding(500, 5, name='province_embeddings')(province_input)\n",
    "province_features = layers.Reshape((5,), name='concat_province')(province_features)\n",
    "\n",
    "variety_features = layers.Embedding(1000, 4, name='variety_embeddings')(variety_input)\n",
    "variety_features = layers.Reshape((4,), name='concat_variety')(variety_features)\n",
    "\n",
    "# Merge all available features into a single large vector via concatenation\n",
    "feature_vector = layers.concatenate([word_features, year_features, country_features, province_features, variety_features, price_input])\n",
    "x = layers.Dropout(0.2)(feature_vector)\n",
    "x = layers.Dense(256, activation='relu', name='Hidden')(x)\n",
    "# Outputs:\n",
    "predictions = layers.Dense(1, name=\"output\")(x)\n",
    "\n",
    "# Instantiate an end-to-end model predicting E,I,O:\n",
    "model = Model(\n",
    "    inputs=[description_input, year_input, country_input, province_input, variety_input, price_input],\n",
    "    outputs=predictions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_dtype=True, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = features_df.sample(frac=0.8, random_state=42)\n",
    "test_set = features_df[~features_df.index.isin(training_set.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(training_set) + len(test_set) == len(wine_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(\n",
    "    {\"description\": training_set[description_cols].values, \n",
    "     \"year\": training_set['year'].values,\n",
    "     \"country\": training_set['country'].values,\n",
    "     \"province\": training_set['province'].values,\n",
    "     \"variety\": training_set['variety'].values, \n",
    "     'price': training_set['price'].values},\n",
    "    \n",
    "    {\"output\": training_set['points'].values},\n",
    "    validation_data=([test_set[description_cols].values, \n",
    "                      test_set['year'].values, \n",
    "                      test_set['country'].values, \n",
    "                      test_set['province'].values, \n",
    "                      test_set['variety'].values, \n",
    "                      test_set['price'].values],\n",
    "                     test_set['points'].values),\n",
    "    epochs=10,\n",
    "    batch_size=512,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['dnn_prediction'] = model.predict({'description': test_set[description_cols], \n",
    "                                            'year': test_set['year'], \n",
    "                                            'country': test_set['country'], \n",
    "                                            'province': test_set['province'], \n",
    "                                            'variety': test_set['variety'], \n",
    "                                            'price': test_set['price']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_prediction_quality(test_set, 'dnn_prediction', target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### description_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_vectors_path = 'Transformers_pickles/descriptions_embeddings_desc.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "description_embeddings = []\n",
    "for i,description in enumerate(df_pos[text_col].values):\n",
    "    description_embeddings.append((i,description, model.encode(description)))\n",
    "    if (i+1) % 1000 == 0:\n",
    "        print(f'Completed step {i+1} out of {len(df_pos)}')\n",
    "        pickle.dump(description_embeddings, open(description_vectors_path, 'wb'))\n",
    "pickle.dump(description_embeddings, open(description_vectors_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(description_embeddings[1300][1] == df_pos.Description.iloc[1300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "descriptions = pickle.load(open('descriptions_embeddings_desc.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rows = []\n",
    "for d in descriptions:\n",
    "    vector = []\n",
    "    vector.append(d[0])\n",
    "    vector.append(d[1])\n",
    "    for item in d[2]:\n",
    "        vector.append(item)\n",
    "    rows.append(vector)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ExtJobTitleText_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_job_title_vectors_path = 'Transformers_pickles/ext_job_title_embeddings_desc.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ext_job_title_embeddings = []\n",
    "for i,description in enumerate(df_pos['ExtJobTitleText'].values):\n",
    "    ext_job_title_embeddings.append((i,description, model.encode(description)))\n",
    "    if (i+1) % 1000 == 0:\n",
    "        print(f'Completed step {i+1} out of {len(df_pos)}')\n",
    "        pickle.dump(ext_job_title_embeddings, open(ext_job_title_vectors_path, 'wb'))\n",
    "pickle.dump(ext_job_title_embeddings, open(ext_job_title_vectors_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(description_embeddings[1300][1] == df_pos.Description.iloc[1300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "descriptions = pickle.load(open('ext_job_title_embeddings_desc.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rows_ext_job_title = []\n",
    "for d in descriptions:\n",
    "    vector = []\n",
    "    vector.append(d[0])\n",
    "    vector.append(d[1])\n",
    "    for item in d[2]:\n",
    "        vector.append(item)\n",
    "    rows_ext_job_title.append(vector)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Concatinating everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* have to concat everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_with_sentence_embeddings_df = pd.DataFrame(rows, columns = ['row_id','description'] + [f'embedding_{i}' for i in range(768)])\n",
    "descriptions_with_sentence_embeddings_df.to_pickle('descriptions_with_sentence_embeddings_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "descriptions_with_sentence_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_with_sentence_embeddings_df = descriptions_with_sentence_embeddings_df.set_index('row_id').join(df_pos['JobTitle_tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "descriptions_with_sentence_embeddings_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = descriptions_with_sentence_embeddings_df[['description','JobTitle_tokenized']].sample()\n",
    "s.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_with_sentence_embeddings_df.query('description == @s.description.iloc[0]')['JobTitle_tokenized']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modedling with transformers sequence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization, Embedding, Dense, GlobalAveragePooling1D, Dropout, Reshape, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col, target_col = 'Desc_concatinated', 'JobTitle_tokenized'\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "label_as_binary = LabelBinarizer()\n",
    "\n",
    "label_as_binary.fit(df_pos[target_col])\n",
    "\n",
    "training = descriptions_with_sentence_embeddings_df.sample(frac=0.8, random_state=41)\n",
    "test = descriptions_with_sentence_embeddings_df[~descriptions_with_sentence_embeddings_df.index.isin(training_set.index)]\n",
    "\n",
    "train__y_labels = label_as_binary.transform(training[target_col])\n",
    "test__y_labels = label_as_binary.transform(test[target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(4096, input_dim=768))\n",
    "model.add(Activation('elu'))\n",
    "# model.add(Dense(2048, input_dim=2048))\n",
    "# model.add(Activation('relu'))\n",
    "model.add(Dense(df_pos.JobTitle.nunique()))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_vector = [f'embedding_{i}' for i in range(768)]\n",
    "history = model.fit(training[f_vector], \n",
    "          train__y_labels, \n",
    "          validation_data=(test[f_vector], test__y_labels), \n",
    "          epochs=7,\n",
    "          batch_size=1024,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Simple NN Prediction & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['token_with_best_prediction'] = model.predict(test[f_vector]).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['prob_token_with_best_prediction'] = model.predict(test[f_vector]).max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "print(precision_score(test['JobTitle_tokenized'],test['token_with_best_prediction'], average=\"macro\"))\n",
    "print(recall_score(test['JobTitle_tokenized'],test['token_with_best_prediction'], average=\"macro\"))    \n",
    "print(accuracy_score(test['JobTitle_tokenized'],test['token_with_best_prediction']))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a loop to see the best metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_vector = [f'embedding_{i}' for i in range(768)]\n",
    "transformer_results2 = [0]*100\n",
    "for i in range(100):\n",
    "    history_loop = model.fit(training[f_vector], \n",
    "                              train__y_labels, \n",
    "                              validation_data=(test[f_vector], test__y_labels), \n",
    "                              epochs=1,\n",
    "                              batch_size=1024,\n",
    "                              verbose=1)\n",
    "    \n",
    "    prob = model.predict(test[f_vector])\n",
    "    test['token_with_best_prediction'] = prob.argmax(axis=1)\n",
    "#     test['prob_token_with_best_prediction'] = prob.max(axis=1)\n",
    "    \n",
    "    transformer_results2[i] = [i,\n",
    "                            precision_score(test['JobTitle_tokenized'],test['token_with_best_prediction'], average=\"weighted\", zero_division=0),\n",
    "                            recall_score(test['JobTitle_tokenized'],test['token_with_best_prediction'], average=\"weighted\", zero_division=0), \n",
    "                            accuracy_score(test['JobTitle_tokenized'],test['token_with_best_prediction'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(transformer_results2, columns=['epoch','val_precision','val_re-call','val_accuracy']).set_index('epoch')[['val_precision','val_re-call']].plot()\n",
    "[0.673, 0.687]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
